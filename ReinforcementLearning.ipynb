{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpGEAM5sYGHjgjDHaYb6gt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swaroopkasaraneni/DatasciencePython/blob/main/ReinforcementLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62GlShDvdpSs",
        "outputId": "7af45953-a45f-4a56-cb70-9e632d487180"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1: Total Reward: -65\n",
            "Episode 2: Total Reward: -18\n",
            "Episode 3: Total Reward: -25\n",
            "Episode 4: Total Reward: -127\n",
            "Episode 5: Total Reward: -28\n",
            "Episode 6: Total Reward: -5\n",
            "Episode 7: Total Reward: -15\n",
            "Episode 8: Total Reward: -18\n",
            "Episode 9: Total Reward: -8\n",
            "Episode 10: Total Reward: -48\n",
            "Episode 11: Total Reward: -45\n",
            "Episode 12: Total Reward: -27\n",
            "Episode 13: Total Reward: -27\n",
            "Episode 14: Total Reward: -35\n",
            "Episode 15: Total Reward: -38\n",
            "Episode 16: Total Reward: -38\n",
            "Episode 17: Total Reward: -35\n",
            "Episode 18: Total Reward: -68\n",
            "Episode 19: Total Reward: -78\n",
            "Episode 20: Total Reward: -5\n",
            "Episode 21: Total Reward: -98\n",
            "Episode 22: Total Reward: -8\n",
            "Episode 23: Total Reward: -17\n",
            "Episode 24: Total Reward: -27\n",
            "Episode 25: Total Reward: -28\n",
            "Episode 26: Total Reward: -47\n",
            "Episode 27: Total Reward: -18\n",
            "Episode 28: Total Reward: -8\n",
            "Episode 29: Total Reward: -17\n",
            "Episode 30: Total Reward: -7\n",
            "Episode 31: Total Reward: -5\n",
            "Episode 32: Total Reward: -8\n",
            "Episode 33: Total Reward: -8\n",
            "Episode 34: Total Reward: -38\n",
            "Episode 35: Total Reward: -28\n",
            "Episode 36: Total Reward: -15\n",
            "Episode 37: Total Reward: -18\n",
            "Episode 38: Total Reward: -5\n",
            "Episode 39: Total Reward: -27\n",
            "Episode 40: Total Reward: -8\n",
            "Episode 41: Total Reward: -7\n",
            "Episode 42: Total Reward: -18\n",
            "Episode 43: Total Reward: -8\n",
            "Episode 44: Total Reward: -7\n",
            "Episode 45: Total Reward: -7\n",
            "Episode 46: Total Reward: -25\n",
            "Episode 47: Total Reward: -48\n",
            "Episode 48: Total Reward: -8\n",
            "Episode 49: Total Reward: -135\n",
            "Episode 50: Total Reward: -48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n",
            "\n",
            "KeyboardInterrupt\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Define a custom gym environment\n",
        "class DeliveryEnv(gym.Env):\n",
        "    def __init__(self, locations, distance_matrix):\n",
        "        super(DeliveryEnv, self).__init__()\n",
        "        self.locations = locations\n",
        "        self.distance_matrix = distance_matrix\n",
        "        self.num_locations = len(locations)\n",
        "\n",
        "        # Action and observation spaces\n",
        "        self.action_space = gym.spaces.Discrete(self.num_locations)  # Choose the next location\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0, high=self.num_locations - 1, shape=(1,), dtype=np.int32\n",
        "        )\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_location = random.randint(0, self.num_locations - 1)\n",
        "        self.visited = set()\n",
        "        self.visited.add(self.current_location)\n",
        "        self.total_cost = 0\n",
        "        return np.array([self.current_location], dtype=np.int32)\n",
        "\n",
        "    def step(self, action):\n",
        "        if action in self.visited or action == self.current_location:\n",
        "            reward = -10  # Penalize revisiting or invalid actions\n",
        "        else:\n",
        "            cost = self.distance_matrix[self.current_location][action]\n",
        "            reward = -cost\n",
        "            self.visited.add(action)\n",
        "            self.total_cost += cost\n",
        "\n",
        "        self.current_location = action\n",
        "\n",
        "        # Check if all locations have been visited\n",
        "        done = len(self.visited) == self.num_locations\n",
        "        return np.array([self.current_location], dtype=np.int32), reward, done, {}\n",
        "\n",
        "    def render(self):\n",
        "        print(f\"Current Location: {self.current_location}, Visited: {self.visited}\")\n",
        "\n",
        "# Example distance matrix\n",
        "locations = [\"A\", \"B\", \"C\"]\n",
        "distance_matrix = [\n",
        "    [0, 2, 5],\n",
        "    [2, 0, 3],\n",
        "    [5, 3, 0],\n",
        "\n",
        "]\n",
        "\n",
        "env = DeliveryEnv(locations, distance_matrix)\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.01\n",
        "gamma = 0.99  # Discount factor\n",
        "\n",
        "# Neural Network Model\n",
        "class PolicyGradientModel(tf.keras.Model):\n",
        "    def __init__(self, num_actions):\n",
        "        super(PolicyGradientModel, self).__init__()\n",
        "        self.hidden = tf.keras.layers.Dense(128, activation='relu')\n",
        "        self.logits = tf.keras.layers.Dense(num_actions)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.hidden(inputs)\n",
        "        return self.logits(x)\n",
        "\n",
        "model = PolicyGradientModel(num_actions=env.action_space.n)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "# Function to get action probabilities\n",
        "def get_action_probs(model, state):\n",
        "    logits = model(tf.convert_to_tensor(state, dtype=tf.float32))\n",
        "    return tf.nn.softmax(logits)\n",
        "\n",
        "# Training Loop\n",
        "def train_model(env, model, episodes=1000):\n",
        "    all_rewards = []\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        states, actions, rewards = [], [], []\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            state_onehot = tf.one_hot(state, depth=env.action_space.n)\n",
        "            action_probs = get_action_probs(model, state_onehot)\n",
        "            action = np.random.choice(env.action_space.n, p=action_probs.numpy()[0])\n",
        "\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            states.append(state_onehot)\n",
        "            actions.append(action)\n",
        "            rewards.append(reward)\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        # Compute discounted rewards\n",
        "        discounted_rewards = []\n",
        "        cumulative_reward = 0\n",
        "        for r in reversed(rewards):\n",
        "            cumulative_reward = r + gamma * cumulative_reward\n",
        "            discounted_rewards.insert(0, cumulative_reward)\n",
        "\n",
        "        # Update model\n",
        "        with tf.GradientTape() as tape:\n",
        "            state_tensor = tf.concat(states, axis=0)\n",
        "            logits = model(state_tensor)\n",
        "            action_masks = tf.one_hot(actions, env.action_space.n)\n",
        "            log_probs = tf.reduce_sum(action_masks * tf.nn.log_softmax(logits), axis=1)\n",
        "            loss = -tf.reduce_mean(log_probs * discounted_rewards)\n",
        "\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "        all_rewards.append(sum(rewards))\n",
        "        print(f\"Episode {episode + 1}: Total Reward: {sum(rewards)}\")\n",
        "\n",
        "    return all_rewards\n",
        "\n",
        "# Train the policy gradient model\n",
        "rewards = train_model(env, model, episodes=50)\n",
        "\n",
        "# Evaluate the trained model\n",
        "state = env.reset()\n",
        "done = False\n",
        "path = [state[0]]\n",
        "\n",
        "while not done:\n",
        "    state_onehot = tf.one_hot(state, depth=env.action_space.n)\n",
        "    action_probs = get_action_probs(model, state_onehot)\n",
        "    action = np.argmax(action_probs.numpy()[0])\n",
        "    state, _, done, _ = env.step(action)\n",
        "    path.append(state[0])\n",
        "\n",
        "print(\"Optimal Path:\", [locations[i] for i in path])\n"
      ]
    }
  ]
}